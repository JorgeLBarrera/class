{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter notebooks\n",
    "\n",
    "This is a [Jupyter](http://jupyter.org/) notebook using Python.  You can install Jupyter locally to edit and interact with this notebook.\n",
    "\n",
    "# Sparse and iterative methods\n",
    "\n",
    "Many matrices in applications, particularly the study of physical systems and graphs/networks, have entries that are mostly equal to zero.  We can more efficiently store such systems by storing only the nonzero elements.  We will discuss storage and optimized implementations later.  Many of the methods for sparse systems apply to solving systems with matrices $A$ that can be applied to a vector ($y \\gets A x$) in significantly less than $O(n^2)$ complexity, or that are \"well-conditioned\" such that an iterative method converges in significantly less than $n$ iterations.\n",
    "\n",
    "[PETSc](https://mcs.anl.gov/petsc), the Portable Extensible Toolkit for Scientific computing, is an open source software package for the parallel solution of algebraic and differential-algebraic equations.  This includes linear algebra, for which PETSc offers a broad variety of implementations.  For general information about PETSc, I refer to [this primer](https://jedbrown.org/files/20150924-PETScPrimer.pdf).\n",
    "\n",
    "## Convergence of stationary iterative methods\n",
    "\n",
    "### Richardson iteration\n",
    "The simplest iterative method is [Richardson's method](https://en.wikipedia.org/wiki/Modified_Richardson_iteration), which solves $A x = b$ by the iteration\n",
    "$$ x_{k+1} = x_k + \\omega (b - A x_k) $$\n",
    "where $\\omega > 0$ is a damping parameter and $x_0$ is an initial guess (possibly the zero vector).  If $b = A x_*$, this iteration is equivalent to\n",
    "$$ x_{k+1} - x_* = (x_k - x_*) - \\omega A (x_k - x_*) = (I - \\omega A) (x_k - x_*) .$$\n",
    "It is convenient for convergence analysis to identify the \"error\" $e_k = x_k - x_*$, in which this becomes\n",
    "$$ e_{k+1} = (I - \\omega A) e_k $$\n",
    "or\n",
    "$$ e_k = (I - \\omega A)^k e_0 $$\n",
    "in terms of the initial error.  Evidently powers of the *iteration matrix* $I - \\omega A$ tell the whole story.\n",
    "Suppose that the eigendecomposition\n",
    "$$ X \\Lambda X^{-1} = I - \\omega A $$\n",
    "exists.  Then\n",
    "$$ (I - \\omega A)^k = (X \\Lambda X^{-1})^k = X \\Lambda^k X^{-1} $$\n",
    "and the convergence (or divergence) rate depends only on the largest magnitude eigenvalue.\n",
    "This analysis isn't great for two reasons:\n",
    "\n",
    "1. Not all matrices are diagonalizable.\n",
    "2. The matrix $X$ may be very ill-conditioned.\n",
    "\n",
    "We can repair these weaknesses by using the [Schur decomposition](https://en.wikipedia.org/wiki/Schur_decomposition)\n",
    "$$ Q R Q^h = I - \\omega A $$\n",
    "where $R$ is right-triangular and $Q$ is unitary (i.e., orthogonal if real-valued; $Q^h$ is the Hermitian conjugate of $Q$).\n",
    "The Schur decomposition always exists and $Q$ has a condition number of 1.\n",
    "\n",
    "* Where are the eigenvalues in $R$?\n",
    "\n",
    "Evidently we must find $\\omega$ to minimize the maximum eigenvalue of $I - \\omega A$.  We can do this if $A$ is well conditioned, but not in general.\n",
    "\n",
    "### Preconditioning\n",
    "\n",
    "Preconditioning is the act of creating an \"affordable\" operation \"$P^{-1}$\" such that $P^{-1} A$ (or $A P^{-1}$) is is well-conditoned or otherwise has a \"nice\" spectrum.  We then solve the system\n",
    "\n",
    "$$ P^{-1} A x = P^{-1} b \\quad \\text{or}\\quad A P^{-1} \\underbrace{(P x)}_y = b $$\n",
    "\n",
    "in which case the convergence rate depends on the spectrum of the iteration matrix\n",
    "$$ I - \\omega P^{-1} A . $$\n",
    "\n",
    "* The preconditioner must be applied on each iteration.\n",
    "* It is *not* merely about finding a good initial guess.\n",
    "\n",
    "There are two complementary techniques necessary for efficient iterative methods:\n",
    "\n",
    "* \"accelerators\" or Krylov methods, which use orthogonality to adaptively converge faster than Richardson\n",
    "* preconditioners that improve the spectrum of the preconditioned operator\n",
    "\n",
    "Although there is ongoing research in Krylov methods and they are immensely useful, I would say preconditioning is 90% of the game for practical applications, particularly as a research area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
